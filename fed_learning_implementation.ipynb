{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qZTrRlBSq4Lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455085e2-98b3-4d11-a7d3-e2684143b1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "#define hyperparameters\n",
        "num_clients = 10\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "\n",
        "def ar_list(w1_np_array):\n",
        "  w1_shard1 = np.reshape(w1_np_array[0:288], (3,3,1,32))\n",
        "  w1_shard2 = np.reshape(w1_np_array[288:320], (32,))\n",
        "  w1_shard3 = np.reshape(w1_np_array[320:18752], (3,3,32,64))\n",
        "  w1_shard4 = np.reshape(w1_np_array[18752:18816], (64,))\n",
        "  w1_shard5 = np.reshape(w1_np_array[18816:34816], (1600,10))\n",
        "  w1_shard6 = np.reshape(w1_np_array[34816:34826], (10,))\n",
        "  w1_list = [w1_shard1,w1_shard2,w1_shard3,w1_shard4,w1_shard5,w1_shard6]\n",
        "  return w1_list\n",
        "\n",
        "# Split data into shards for each client\n",
        "data_per_client = len(x_train) // num_clients\n",
        "x_train_shards = np.split(x_train[:num_clients * data_per_client], num_clients)\n",
        "y_train_shards = np.split(y_train[:num_clients * data_per_client], num_clients)\n",
        "\n",
        "def ar_weights(w1):\n",
        "  w1_reshaped = [np.reshape(w, (-1, 1)) for w in w1]\n",
        "  # Concatenate the reshaped arrays\n",
        "  w1_array = np.concatenate(w1_reshaped)\n",
        "  #  Convert the w1_array to a NumPy array\n",
        "  w1_np_array = np.array(w1_array)\n",
        "  return w1_np_array\n",
        "\n",
        "#algorithm 1, mifl update\n",
        "# Define some hyperparameters\n",
        "E = 1 # Number of local epochs\n",
        "B = 32 # Batch size\n",
        "h = 0.01 # Learning rate\n",
        "s = 0.5 # Threshold for cv\n",
        "lamb = 0#lambda\n",
        "omega = 0\n",
        "def create_model():\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=(28, 28, 1)),\n",
        "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(10, activation=\"softmax\"),\n",
        "      ]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# Initialize global model\n",
        "global_model = create_model()\n",
        "global_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(h), metrics=[\"accuracy\"])\n",
        "global_model.set_weights(ar_list(np.random.rand(34826, 1)))\n",
        "\n",
        "def mutual_information_score(y_true, y_pred):\n",
        "  # Compute the mutual information between the true labels and the predictions\n",
        "  mi = mutual_info_score(y_true, y_pred)\n",
        "  # Return the mutual information\n",
        "  return mi\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "'''def estimate_pdf(weights):\n",
        "    # This function estimates the probability density function using Gaussian Kernel Density Estimation\n",
        "    kde = gaussian_kde(weights)\n",
        "    return kde\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    # This function calculates the Kullback-Leibler divergence between two distributions\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def mutual_information_score(weights1, weights2):\n",
        "    # Estimate the PDFs for both sets of weights\n",
        "    pdf1 = estimate_pdf(weights1)\n",
        "    pdf2 = estimate_pdf(weights2)\n",
        "\n",
        "    # Evaluate the PDFs on a grid of points (for simplicity, we assume 1D weights)\n",
        "    grid_points = np.linspace(min(min(weights1), min(weights2)), max(max(weights1), max(weights2)), num=100)\n",
        "    p1 = pdf1(grid_points)\n",
        "    p2 = pdf2(grid_points)\n",
        "\n",
        "    # Calculate the joint distribution as the outer product of the two PDFs\n",
        "    joint_pdf = np.outer(p1, p2)\n",
        "\n",
        "    # Calculate the product of the marginals\n",
        "    product_marginals = p1[:, None] * p2[None, :]\n",
        "\n",
        "    # Calculate the mutual information\n",
        "    mi = kl_divergence(joint_pdf.ravel(), product_marginals.ravel())\n",
        "\n",
        "    return mi'''\n",
        "\n",
        "\n",
        "#list for storing current local models\n",
        "local_models = []\n",
        "\n",
        "def cv_fn(lg, lk):\n",
        "  # Use the formula from the paper to calculate cv\n",
        "  '''lg_mean = tf.reduce_mean(lg, axis=0)\n",
        "  lk_mean = tf.reduce_mean(lk, axis=0)\n",
        "  lg_var = tf.math.reduce_variance(lg, axis=0)\n",
        "  lk_var = tf.math.reduce_variance(lk, axis=0)\n",
        "  lg_lk_cov = tf.reduce_mean((lg - lg_mean) * (lk - lk_mean), axis=0)\n",
        "  rho = (lg_var + lk_var - 2 * lg_lk_cov) / ((lg_var * lk_var)**2)'''\n",
        "  correlation_matrix = np.corrcoef(lg, lk)\n",
        "  rho = correlation_matrix[0, 1]\n",
        "  return rho\n",
        "\n",
        "def mi_fn(lg, lk):\n",
        "  # Use the formula from the paper to calculate cv\n",
        "  '''lg_mean = tf.reduce_mean(lg, axis=0)\n",
        "  lk_mean = tf.reduce_mean(lk, axis=0)\n",
        "  lg_var = tf.math.reduce_variance(lg, axis=0)\n",
        "  lk_var = tf.math.reduce_variance(lk, axis=0)\n",
        "  lg_lk_cov = tf.reduce_mean((lg - lg_mean) * (lk - lk_mean), axis=0)\n",
        "  rho = (lg_var + lk_var - 2 * lg_lk_cov) / ((lg_var * lk_var)**2)'''\n",
        "  correlation_matrix = np.corrcoef(lg, lk)\n",
        "  rho = correlation_matrix[0, 1]\n",
        "  mi = -0.5 * tf.math.log(1-rho**2)\n",
        "  return mi\n",
        "\n",
        "\n",
        "\n",
        "def mifl_update(k,t,wt):\n",
        "  if(t == 0):\n",
        "    # Create a local model for the client\n",
        "    Fg = create_model()\n",
        "    Fg.set_weights(global_model.get_weights())\n",
        "    Fg_pred_array = np.argmax(Fg.predict(x_train_shards[k]), axis=1)\n",
        "    lg = Fg_pred_array - y_train_shards[k]\n",
        "\n",
        "    Fk = create_model()\n",
        "    Fk.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    # Copy the global model weights to the local model\n",
        "    #Fg.set_weights(global_model.get_weights())\n",
        "    Fk.fit(x_train_shards[k], y_train_shards[k], batch_size=B, epochs=E, verbose=0)\n",
        "    # Add the local model to the dict\n",
        "    local_models.append(Fk.get_weights())\n",
        "    Fk_pred_array = np.argmax(Fk.predict(x_train_shards[k]), axis=1)\n",
        "    lk = Fk_pred_array - y_train_shards[k]\n",
        "\n",
        "    mi = mi_fn(lg,lk)\n",
        "    MI.append(mi)\n",
        "\n",
        "  else:\n",
        "    Fk = create_model()\n",
        "    Fk.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    Fk.set_weights(local_models[k])\n",
        "    Fg = create_model()\n",
        "    Fg.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    weights = global_model.get_weights()\n",
        "    Fg.set_weights(weights)\n",
        "    #currently not taking batches into account\n",
        "    for i in range (E):\n",
        "      '''range is 6000 because of number of samples in each x_train_shard,y_train_shard, change it accordingly when batches have been taken into account\n",
        "      for j in range(0,6000):\n",
        "          lg = Fg.predict(x_train_shards[j]) - y_train_shards[j]\n",
        "          lk = Fk.predict(y_train_shards[j]) - y_train_shards[j]\n",
        "      '''\n",
        "      Fg_pred_array = np.argmax(Fg.predict(x_train_shards[k]), axis=1)\n",
        "      lg = Fg_pred_array - y_train_shards[k]\n",
        "\n",
        "      Fk_pred_array = np.argmax(Fk.predict(x_train_shards[k]), axis=1)\n",
        "      lk = Fk_pred_array - y_train_shards[k]\n",
        "\n",
        "      cv = cv_fn(lg,lk)\n",
        "      #cv = mutual_information_score(lg,lk)\n",
        "      if np.array_equal(lg, lk):\n",
        "        lamb = 1/2\n",
        "      elif cv <= 1/2:\n",
        "        lamb = cv\n",
        "      else:\n",
        "        lamb = 1 - cv\n",
        "      omega = (1-lamb)*lg + lamb*lk\n",
        "\n",
        "      local_weight = ar_weights(Fk.get_weights())\n",
        "      # update local models\n",
        "      local_weight = local_weight - h*omega\n",
        "      local_weight = ar_list(local_weight)\n",
        "\n",
        "    Fk.set_weights(local_weight)\n",
        "    local_models[k] = Fk.get_weights()\n",
        "    Fg_pred_array = np.argmax(Fg.predict(x_train_shards[k]), axis=1)\n",
        "    lg = Fg_pred_array - y_train_shards[k]\n",
        "    Fk_pred_array = np.argmax(Fk.predict(x_train_shards[k]), axis=1)\n",
        "    lk = Fk_pred_array - y_train_shards[k]\n",
        "    cv = cv_fn(lg,lk) # cv for updated model\n",
        "    #mi = mi_fn(lg,lk)\n",
        "    mi = -0.5 * tf.math.log(1-cv**2)\n",
        "    MI[k] = mi\n",
        "    return local_weight,mi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(Fg.predict(x_train_shards[k]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "lrruF2YWnxvZ",
        "outputId": "ba7d615c-ead7-4624-d455-456b355ce898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Fg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-894e8c11d306>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_shards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Fg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QXMg2NjVtdZC",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2d7aa9fa-81bc-42fd-a1d7-90ebc4aaee5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 3s 14ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 2s 9ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n",
            "188/188 [==============================] - 2s 11ms/step\n",
            "188/188 [==============================] - 1s 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for k in range (len(local_models)):\\n  wt[i] = local_models[k].get_weights()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import random\n",
        "# Define the MIFL-Aggregation function\n",
        "t_acc = 95#targeted accuracy\n",
        "wt = [global_model.get_weights()]*num_clients\n",
        "MI = []\n",
        "pairs = []\n",
        "ktoplist = []\n",
        "Ktop = len(wt)//3#arbitrary integer for now\n",
        "C = 0.1\n",
        "accuracy = 0\n",
        "n = int(len(wt) * C)\n",
        "def mifl_aggregation(wt, MI, pairs):\n",
        "  ###initialize w0\n",
        "  # Run on FL server\n",
        "  # wt is a list of local model weights from clients\n",
        "  # MI is a list of mutual information values from clients\n",
        "  t = 1 #round\n",
        "  global accuracy\n",
        "  while accuracy < t_acc:\n",
        "    '''selected_elements = random.choices(wt, k=n)\n",
        "    for k in range(len(selected_elements)):\n",
        "      j = selected_elements[k]\n",
        "      wt[j],MI[k] = mifl_update(k,t,wt[k])'''\n",
        "\n",
        "    selected_nodes = random.choices([x for x in range(num_clients)], k=n)\n",
        "    for k in selected_nodes:\n",
        "      wt[k],MI[k] = mifl_update(k,t,wt[k])\n",
        "\n",
        "    for k in selected_nodes:\n",
        "      pairs.append([MI[k],wt[k],k])\n",
        "    sorted_pairs = sorted(pairs, key=lambda x: x[0])\n",
        "    # when running the algorithm for the mnist dataset, we have divided the data such that there is an\n",
        "    # equal number of samples for each client, update code if that is not the case.\n",
        "    for k in range(Ktop):\n",
        "      ktoplist.append(ar_weights(sorted_pairs[k][1]))\n",
        "\n",
        "    averaged_weights = np.average(ktoplist, axis=0)\n",
        "    averaged_weights = ar_list(averaged_weights)\n",
        "    #for k in range(Ktop,len(sorted_pairs)):\n",
        "     # sorted_pairs[k][1] = averaged_weights\n",
        "    #pairs = sorted(sorted_pairs, key = lambda x: x[2])\n",
        "    global_model.set_weights(averaged_weights)\n",
        "    accuracy = global_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "    t = t + 1\n",
        "\n",
        "for i in range (num_clients):\n",
        "  mifl_update(i,0,wt)\n",
        "  wt[i] = local_models[i]\n",
        "\n",
        "\n",
        "\n",
        "'''for k in range (len(local_models)):\n",
        "  wt[i] = local_models[k].get_weights()'''\n",
        "\n",
        "#problems currently identified in second algo-\n",
        "#initialize w0 not done, what do i initialize them with?\n",
        "#Ktop arbitrarily taken, step 9 -TEMP FIX\n",
        "#assumed each client has equal number of samples -TEMP FIX\n",
        "#once sorted, take them back to old position -SOLVED\n",
        "#how is accuracy being updated, not mentioned in the algo -SOLVED\n",
        "#prevent overwriting\n",
        "#problems in first algorithm\n",
        "#didnt take batches into account\n",
        "#have to see how to integrate the first and the second\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mifl_aggregation(wt, MI, pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "km3TpZG7Q-sY",
        "outputId": "5d86e6f7-0fde-43ba-8a14-5f6e2f54d61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 8ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'predict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-15752c4417c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmifl_aggregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-108d4054dadb>\u001b[0m in \u001b[0;36mmifl_aggregation\u001b[0;34m(wt, MI, pairs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mselected_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mwt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmifl_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-437b657200d8>\u001b[0m in \u001b[0;36mmifl_update\u001b[0;34m(k, t, wt)\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0mlg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFg_pred_array\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train_shards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       \u001b[0mFk_pred_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_shards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m       \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFk_pred_array\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train_shards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0gN6b_BtGWz"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "#define hyperparameters\n",
        "num_clients = 10\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "\n",
        "# Split data into shards for each client\n",
        "data_per_client = len(x_train) // num_clients\n",
        "x_train_shards = np.split(x_train[:num_clients * data_per_client], num_clients)\n",
        "y_train_shards = np.split(y_train[:num_clients * data_per_client], num_clients)\n",
        "\n",
        "#algorithm 1, mifl update\n",
        "# Define some hyperparameters\n",
        "E = 10 # Number of local epochs\n",
        "B = 32 # Batch size\n",
        "h = 0.01 # Learning rate\n",
        "s = 0.5 # Threshold for cv\n",
        "lamb = 0#lambda\n",
        "omega = 0\n",
        "def create_model():\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=(28, 28, 1)),\n",
        "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(10, activation=\"softmax\"),\n",
        "      ]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# Initialize global model\n",
        "global_model = create_model()\n",
        "global_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(h), metrics=[\"accuracy\"])\n",
        "\n",
        "def mutual_information_score(y_true, y_pred):\n",
        "  # Compute the mutual information between the true labels and the predictions\n",
        "  mi = mutual_info_score(y_true, y_pred)\n",
        "  # Return the mutual information\n",
        "  return mi\n",
        "#list for storing current local models\n",
        "local_models = [0]*num_clients\n",
        "\n",
        "def cv_fn(lg, lk):\n",
        "  # Use the formula from the paper to calculate cv\n",
        "  lg_mean = tf.reduce_mean(lg, axis=0)\n",
        "  lk_mean = tf.reduce_mean(lk, axis=0)\n",
        "  lg_var = tf.math.reduce_variance(lg, axis=0)\n",
        "  lk_var = tf.math.reduce_variance(lk, axis=0)\n",
        "  lg_lk_cov = tf.reduce_mean((lg - lg_mean) * (lk - lk_mean), axis=0)\n",
        "  rho = (lg_var + lk_var - 2 * lg_lk_cov) / ((lg_var * lk_var)**2)\n",
        "  cv = -0.5 * tf.math.log(1-rho**2)\n",
        "  return cv\n",
        "\n",
        "def mifl_update(k,t,wt):\n",
        "  if(t == 0):\n",
        "    # Create a local model for the client\n",
        "    Fg = create_model()\n",
        "    Fg.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    # Copy the global model weights to the local model\n",
        "    Fg.set_weights(global_model.get_weights())\n",
        "    Fg.fit(x_train_shards[k], y_train_shards[k], batch_size=B, epochs=E, verbose=0)\n",
        "    # Add the local model to the dict\n",
        "    local_models[k] = Fg\n",
        "  else:\n",
        "    Fk = local_models[k]\n",
        "    Fg = create_model()\n",
        "    Fg.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    weights = global_model.get_weights()\n",
        "    Fg.set_weights(weights)\n",
        "    #currently not taking batches into account\n",
        "    for i in range (E):\n",
        "      '''range is 6000 because of number of samples in each x_train_shard,y_train_shard, change it accordingly when batches have been taken into account\n",
        "      for j in range(0,6000):\n",
        "          lg = Fg.predict(x_train_shards[j]) - y_train_shards[j]\n",
        "          lk = Fk.predict(y_train_shards[j]) - y_train_shards[j]\n",
        "      '''\n",
        "      lg = Fg.predict(x_train_shards) - y_train_shards\n",
        "      lk = Fk.predict(x_train_shards) - y_train_shards\n",
        "      cv = cv_fn(lg,lk)\n",
        "      #cv = mutual_information_score(lg,lk)\n",
        "      if lg == lk:\n",
        "        lamb = 1/2\n",
        "      elif cv <= 1/2:\n",
        "        lamb = cv\n",
        "      else:\n",
        "        lamb = 1 - cv\n",
        "      omega = (1-lamb)*lg + lamb*lk\n",
        "      weights = weights - h*omega\n",
        "    #prediction = model.predict(input_sample)\n",
        "    Fg.set_weights(weights)\n",
        "    mi = mutual_information_score(Fg.get_weights(),Fk.get_weights())\n",
        "    return weights,mi\n",
        "\n",
        "import random\n",
        "# Define the MIFL-Aggregation function\n",
        "t_acc = 95 #targeted accuracy\n",
        "wt = [0]*num_clients\n",
        "MI = []\n",
        "pairs = []\n",
        "ktoplist = []\n",
        "Ktop = len(wt)//3#arbitrary integer for now\n",
        "C = 0.1\n",
        "accuracy = 0\n",
        "n = int(len(wt) * C)\n",
        "def mifl_aggregation(wt, MI, pairs):\n",
        "  ###initialize w0\n",
        "  # Run on FL server\n",
        "  # wt is a list of local model weights from clients\n",
        "  # MI is a list of mutual information values from clients\n",
        "  t = 0 #round\n",
        "  global accuracy\n",
        "  while accuracy < t_acc:\n",
        "    selected_elements = random.choices(wt, k=n)\n",
        "    for k in range(len(selected_elements)):\n",
        "      wt[k],MI[k] = mifl_update(k,t,wt[k])\n",
        "      t = t + 1\n",
        "    for k in range(len(wt)):\n",
        "      pairs[k] = [MI[k],wt[k],k]\n",
        "    sorted_pairs = sorted(pairs, key=lambda x: x[0])\n",
        "    # when running the algorithm for the mnist dataset, we have divided the data such that there is an\n",
        "    # equal number of samples for each client, update code if that is not the case.\n",
        "    for k in range(Ktop):\n",
        "      ktoplist.append(sorted_pairs[k][1])\n",
        "    averaged_weights = np.average(ktoplist, axis=0)\n",
        "    for k in range(Ktop,len(sorted_pairs)):\n",
        "      sorted_pairs[k][1] = averaged_weights\n",
        "    pairs = sorted(sorted_pairs, key = lambda x: x[2])\n",
        "    global_model.set_weights(averaged_weights)\n",
        "    accuracy = global_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "    print(accuracy)\n",
        "mifl_aggregation(wt, MI, pairs)\n",
        "for i in range (num_clients):\n",
        "  mifl_update(i,0,wt)\n",
        "  wt.append(local_models[i].get_weights())\n",
        "for k in range (len(local_models)):\n",
        "  wt.append(local_models[k].get_weights())\n",
        "\n",
        "#problems currently identified in second algo-\n",
        "#initialize w0 not done, what do i initialize them with?\n",
        "#Ktop arbitrarily taken, step 9 -TEMP FIX\n",
        "#assumed each client has equal number of samples -TEMP FIX\n",
        "#once sorted, take them back to old position -SOLVED\n",
        "#how is accuracy being updated, not mentioned in the algo -SOLVED\n",
        "#prevent overwriting\n",
        "#problems in first algorithm\n",
        "#didnt take batches into account\n",
        "#have to see how to integrate the first and the second\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "SMV_IHLFTP2L",
        "outputId": "de979053-ca00-4fe4-ba71-67d566c66ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d45c2932463b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "#define hyperparameters\n",
        "num_clients = 10\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
        "\n",
        "# Split data into shards for each client\n",
        "data_per_client = len(x_train) // num_clients\n",
        "x_train_shards = np.split(x_train[:num_clients * data_per_client], num_clients)\n",
        "y_train_shards = np.split(y_train[:num_clients * data_per_client], num_clients)\n",
        "\n",
        "#algorithm 1, mifl update\n",
        "# Define some hyperparameters\n",
        "E = 10 # Number of local epochs\n",
        "B = 32 # Batch size\n",
        "h = 0.01 # Learning rate\n",
        "s = 0.5 # Threshold for cv\n",
        "lamb = 0#lambda\n",
        "omega = 0\n",
        "def create_model():\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=(28, 28, 1)),\n",
        "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(10, activation=\"softmax\"),\n",
        "      ]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# Initialize global model\n",
        "global_model = create_model()\n",
        "global_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(h), metrics=[\"accuracy\"])\n",
        "\n",
        "'''def mutual_information_score(y_true, y_pred):\n",
        "  # Compute the mutual information between the true labels and the predictions\n",
        "  mi = mutual_info_score(y_true, y_pred)\n",
        "  # Return the mutual information\n",
        "  return mi'''\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "\n",
        "def estimate_pdf(weights):\n",
        "    # This function estimates the probability density function using Gaussian Kernel Density Estimation\n",
        "    kde = gaussian_kde(weights)\n",
        "    return kde\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    # This function calculates the Kullback-Leibler divergence between two distributions\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def mutual_information_score(weights1, weights2):\n",
        "    # Estimate the PDFs for both sets of weights\n",
        "    pdf1 = estimate_pdf(weights1)\n",
        "    pdf2 = estimate_pdf(weights2)\n",
        "\n",
        "    # Evaluate the PDFs on a grid of points (for simplicity, we assume 1D weights)\n",
        "    grid_points = np.linspace(min(min(weights1), min(weights2)), max(max(weights1), max(weights2)), num=100)\n",
        "    p1 = pdf1(grid_points)\n",
        "    p2 = pdf2(grid_points)\n",
        "\n",
        "    # Calculate the joint distribution as the outer product of the two PDFs\n",
        "    joint_pdf = np.outer(p1, p2)\n",
        "\n",
        "    # Calculate the product of the marginals\n",
        "    product_marginals = p1[:, None] * p2[None, :]\n",
        "\n",
        "    # Calculate the mutual information\n",
        "    mi = kl_divergence(joint_pdf.ravel(), product_marginals.ravel())\n",
        "\n",
        "    return mi\n",
        "\n",
        "\n",
        "#list for storing current local models\n",
        "local_models = [0]*(num_clients+1)\n",
        "\n",
        "def cv_fn(lg, lk):\n",
        "  # Use the formula from the paper to calculate cv\n",
        "  lg_mean = tf.reduce_mean(lg, axis=0)\n",
        "  lk_mean = tf.reduce_mean(lk, axis=0)\n",
        "  lg_var = tf.math.reduce_variance(lg, axis=0)\n",
        "  lk_var = tf.math.reduce_variance(lk, axis=0)\n",
        "  lg_lk_cov = tf.reduce_mean((lg - lg_mean) * (lk - lk_mean), axis=0)\n",
        "  rho = (lg_var + lk_var - 2 * lg_lk_cov) / ((lg_var * lk_var)**2)\n",
        "  cv = -0.5 * tf.math.log(1-rho**2)\n",
        "  return cv\n",
        "\n",
        "def mifl_update(k,t,wt):\n",
        "  if(t == 0):\n",
        "    # Create a local model for the client\n",
        "    Fg = create_model()\n",
        "    Fg.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    # Copy the global model weights to the local model\n",
        "    #Fg.set_weights(global_model.get_weights())\n",
        "    Fg.fit(x_train_shards[k], y_train_shards[k], batch_size=B, epochs=E, verbose=0)\n",
        "    # Add the local model to the dict\n",
        "    local_models[k] = Fg\n",
        "    weights = Fg.get_weights()\n",
        "    mi = 0\n",
        "  else:\n",
        "    Fk = local_models[k]\n",
        "    Fg = create_model()\n",
        "    Fg.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\n",
        "    weights = global_model.get_weights()\n",
        "    Fg.set_weights(weights)\n",
        "    #currently not taking batches into account\n",
        "    for i in range (E):\n",
        "      '''range is 6000 because of number of samples in each x_train_shard,y_train_shard, change it accordingly when batches have been taken into account\n",
        "      for j in range(0,6000):\n",
        "          lg = Fg.predict(x_train_shards[j]) - y_train_shards[j]\n",
        "          lk = Fk.predict(y_train_shards[j]) - y_train_shards[j]\n",
        "      '''\n",
        "      lg = Fg.predict(x_train_shards) - y_train_shards\n",
        "      lk = Fk.predict(x_train_shards) - y_train_shards\n",
        "      cv = cv_fn(lg,lk)\n",
        "      #cv = mutual_information_score(lg,lk)\n",
        "      if lg == lk:\n",
        "        lamb = 1/2\n",
        "      elif cv <= 1/2:\n",
        "        lamb = cv\n",
        "      else:\n",
        "        lamb = 1 - cv\n",
        "      omega = (1-lamb)*lg + lamb*lk\n",
        "      weights = weights - h*omega\n",
        "    #prediction = model.predict(input_sample)\n",
        "    Fg.set_weights(weights)\n",
        "    mi = mutual_information_score(Fg.get_weights(),Fk.get_weights())\n",
        "    print(type(mi))\n",
        "    print(type(weights))\n",
        "  return weights,mi\n",
        "  import random\n",
        "# Define the MIFL-Aggregation function\n",
        "t_acc = 95#targeted accuracy\n",
        "wt = [0]*num_clients\n",
        "MI = []\n",
        "pairs = []\n",
        "ktoplist = []\n",
        "Ktop = len(wt)//3#arbitrary integer for now\n",
        "C = 0.1\n",
        "accuracy = 0\n",
        "n = int(len(wt) * C)\n",
        "def mifl_aggregation(wt, MI, pairs):\n",
        "  ###initialize w0\n",
        "  # Run on FL server\n",
        "  # wt is a list of local model weights from clients\n",
        "  # MI is a list of mutual information values from clients\n",
        "  t = 0 #round\n",
        "  global accuracy\n",
        "  while accuracy < t_acc:\n",
        "    selected_elements = random.choices(wt, k=n)\n",
        "    for k in range(num_clients):\n",
        "      wt[k],MI[k] = mifl_update(k,t,wt[k])\n",
        "    t = t + 1\n",
        "    for k in range(len(wt)):\n",
        "      pairs[k] = [MI[k],wt[k],k]\n",
        "    sorted_pairs = sorted(pairs, key=lambda x: x[0])\n",
        "    # when running the algorithm for the mnist dataset, we have divided the data such that there is an\n",
        "    # equal number of samples for each client, update code if that is not the case.\n",
        "    for k in range(Ktop):\n",
        "      ktoplist.append(sorted_pairs[k][1])\n",
        "    averaged_weights = np.average(ktoplist, axis=0)\n",
        "    for k in range(Ktop,len(sorted_pairs)):\n",
        "      sorted_pairs[k][1] = averaged_weights\n",
        "    pairs = sorted(sorted_pairs, key = lambda x: x[2])\n",
        "    global_model.set_weights(averaged_weights)\n",
        "    accuracy = global_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "    print(accuracy)\n",
        "mifl_aggregation(wt, MI, pairs)\n",
        "for i in range (num_clients):\n",
        "  mifl_update(i,0,wt)\n",
        "  print(\"first round done\")\n",
        "  wt[i] = local_models[i].get_weights()\n",
        "for k in range (len(local_models)):\n",
        "  wt[i] = local_models[k].get_weights()\n",
        "\n",
        "#problems currently identified in second algo-\n",
        "#initialize w0 not done, what do i initialize them with?\n",
        "#Ktop arbitrarily taken, step 9 -TEMP FIX\n",
        "#assumed each client has equal number of samples -TEMP FIX\n",
        "#once sorted, take them back to old position -SOLVED\n",
        "#how is accuracy being updated, not mentioned in the algo -SOLVED\n",
        "#prevent overwriting\n",
        "#problems in first algorithm\n",
        "#didnt take batches into account\n",
        "#have to see how to integrate the first and the second\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0yCE40OvEAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}